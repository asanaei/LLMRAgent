---
title: "LLMRAgent quickstart"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LLMRAgent quickstart}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  eval = identical(Sys.getenv("LLMRAgent_RUN_VIGNETTES"), "true") &&
         nzchar(Sys.getenv("OPENAI_API_KEY")),
  collapse = TRUE, comment = "#>"
)
```

```{r setup}
library(LLMRAgent)
```

This vignette demonstrates the LLMRAgent package which provides a simple 
interface to create AI agents using LLMR model configurations.

## Key Features

- **Direct LLMR Integration**: Uses `LLMR::llm_config()` directly
- **JSON Mode by Default**: Supports structured responses  
- **Memory Management**: Built-in conversation history
- **No Complex Adapters**: Clean, simple interface

## Basic Usage

All agents require a valid LLMR model configuration:

```{r basic_usage, eval=FALSE}
# Requires OPENAI_API_KEY environment variable
config <- LLMR::llm_config(
  provider = "openai", 
  model = "gpt-4o-mini",
  api_key = Sys.getenv("OPENAI_API_KEY")
)

agent <- new_agent(
  system_prompt = "You are a helpful assistant. Be concise.",
  model_config = config
)

agent_reply(agent, "Hello!")
```

## Live Example

Let's create a working example if API key is available:

```{r live_example}
if (requireNamespace("LLMR", quietly = TRUE) && 
    Sys.getenv("OPENAI_API_KEY") != "") {
  
  # Create LLMR config
  config <- LLMR::llm_config(
    provider = "openai",
    model = "gpt-4o-mini", 
    api_key = Sys.getenv("OPENAI_API_KEY")
  )
  
  # Create agent
  agent <- new_agent(
    system_prompt = "You are a helpful R programming assistant. Be very concise.",
    model_config = config
  )
  
  # Single interaction
  cat("=== Basic Interaction ===\n")
  reply1 <- agent_reply(agent, "What is R in one sentence?", json = FALSE)
  cat("User: What is R in one sentence?\n")
  cat("Agent:", reply1, "\n\n")
  
  # JSON mode example
  cat("=== JSON Mode Example ===\n")
  reply2 <- agent_reply(agent, 
    "List 3 benefits of R. Format as JSON with keys: benefit1, benefit2, benefit3", 
    json = TRUE)
  cat("User: List 3 benefits of R as JSON\n")
  cat("Agent (JSON):", reply2, "\n\n")
  
  # Try parsing the JSON
  tryCatch({
    parsed <- jsonlite::fromJSON(reply2)
    cat("Parsed JSON structure:\n")
    print(parsed)
  }, error = function(e) {
    cat("JSON parsing note:", e$message, "\n")
  })
  
} else {
  cat("API key not available. Set OPENAI_API_KEY to see live examples.\n")
  cat("The package requires LLMR::llm_config() with valid credentials.\n")
}
```

## Token Usage Tracking

Agents automatically track token usage across all interactions:

```{r token_usage}
if (requireNamespace("LLMR", quietly = TRUE) && 
    Sys.getenv("OPENAI_API_KEY") != "") {
  
  cat("=== Stateful Token Usage Tracking ===\n")
  
  # Make several calls - usage is tracked automatically
  agent_reply(agent, "What is R?", json = FALSE)
  agent_reply(agent, "What is Python?", json = FALSE) 
  agent_reply(agent, "Compare R and Python in 20 words", json = FALSE)
  
  # Check cumulative usage
  usage <- agent_usage(agent)
  cat("Total interactions:", usage$interactions, "\n")
  cat("Total tokens consumed:", usage$total_tokens, "\n")
  cat("Input tokens:", usage$total_tokens_in, "\n")
  cat("Output tokens:", usage$total_tokens_out, "\n")
  
  # Show interaction history
  cat("\nInteraction history:\n")
  for (i in seq_along(usage$history)) {
    interaction <- usage$history[[i]]
    cat(sprintf("%d. [%s] %d tokens: %s...\n", 
                i, 
                format(interaction$timestamp, "%H:%M:%S"),
                interaction$tokens_total,
                substr(interaction$user_text, 1, 40)))
  }
  
} else {
  cat("Set OPENAI_API_KEY to see token usage tracking.\n")
}
```

## Memory Management

Agents include configurable memory for conversation history:

```{r memory_example}
if (requireNamespace("LLMR", quietly = TRUE) && 
    Sys.getenv("OPENAI_API_KEY") != "") {
  
  cat("=== Memory Example ===\n")
  
  # Create agent with memory
  memory_agent <- new_agent(
    system_prompt = "Remember our conversation. Be very brief.",
    model_config = config,
    memory = new_buffer_memory(4)  # Keep last 4 messages
  )
  
  # Conversation with memory
  r1 <- agent_reply(memory_agent, "My favorite language is R")
  cat("User: My favorite language is R\n")
  cat("Agent:", r1, "\n\n")
  
  r2 <- agent_reply(memory_agent, "What's my favorite language?")  
  cat("User: What's my favorite language?\n")
  cat("Agent:", r2, "\n\n")
  
  # Check memory contents
  memory_msgs <- memory_agent$memory$get()
  cat("Memory contains", length(memory_msgs), "messages:\n")
  for(i in seq_along(memory_msgs)) {
    msg <- memory_msgs[[i]]
    cat(sprintf("%d. [%s]: %s\n", i, msg$role, substr(msg$content, 1, 40)))
  }
  
} else {
  cat("Set OPENAI_API_KEY to see memory examples.\n")
}
```

## Summarization

If you attach a summary memory, it can summarize recent conversation using your LLMR model configuration. You can provide a dedicated summarizer config or let the agent fall back to its main config.

```{r summarization, eval=FALSE}
sm <- new_summary_memory()
agent <- new_agent(
  system_prompt = "Be concise.",
  model_config = config,
  memory = sm,
  summarizer_model_config = config # or a different model for summarization
)

# After a conversation, get a short summary
summary_text <- sm$summary(max_chars = 300)
cat(summary_text)
```

## Built-in Tools

The package includes built-in tools that work offline:

```{r tools}
register_basic_tools(enable_network = FALSE)
available_tools <- list_tools()
cat("Available tools:\n")
if (length(available_tools) > 0) {
  for(i in seq_along(available_tools)) {
    tool <- available_tools[[i]]
    if (is.list(tool)) {
      cat("-", tool$name, ":", tool$description, "\n")
    } else {
      cat("-", names(available_tools)[i], ":", as.character(tool), "\n")
    }
  }
} else {
  cat("No tools registered.\n")
}
```

## Error Handling

The package properly enforces requirements:

```{r error_demo, error=TRUE}
# This will fail - config is required
try(new_agent("Be helpful."))
```

## Summary

The LLMRAgent package provides a clean interface to LLMR:

- **Required**: Valid `LLMR::llm_config()` 
- **Direct integration**: No adapter complexity
- **JSON support**: Built-in structured responses
- **Memory management**: Conversation history
- **Tool system**: Extensible functionality

For more examples, see the `inst/examples/` directory in the package.
